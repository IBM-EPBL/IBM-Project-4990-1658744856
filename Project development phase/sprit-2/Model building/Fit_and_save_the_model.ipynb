{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Real-Time Communication System Powered By AI For Specially Abled\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0e-LVfG86AqW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G-p4jQng5w3T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSXQmLUB6f47",
        "outputId": "39be8548-df5f-41a3-f226-2aca049f8852"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "3-0QOrIs7tFN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Datagen\n",
        "train_datagen = ImageDataGenerator(rescale=1/255,zoom_range=0.2,horizontal_flip=True,vertical_flip=False)\n",
        "# Testing Datagen\n",
        "test_datagen = ImageDataGenerator(rescale=1/255)"
      ],
      "metadata": {
        "id": "CynDfaXV7tQQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Dataset\n",
        "x_train=train_datagen.flow_from_directory(r'/content/gdrive/',target_size=(64,64), class_mode='categorical',batch_size=900)\n",
        "# Testing Dataset\n",
        "x_test=test_datagen.flow_from_directory(r'/content/gdrive/',target_size=(64,64), class_mode='categorical',batch_size=900)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPv6oue67tRX",
        "outputId": "dcc46a6c-1b2e-4a44-f57c-a652b3b30c59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2406 images belonging to 4 classes.\n",
            "Found 2406 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Len x-train : \", len(x_train))\n",
        "print(\"Len x-test : \", len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNv8xgsY7tWh",
        "outputId": "1ec94d7e-daa3-4a8d-ac93-3be3026cbe18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len x-train :  3\n",
            "Len x-test :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Class Indices in Training Dataset\n",
        "x_train.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3nB5vXe8Y8V",
        "outputId": "bbbba06f-86c2-48fe-8b07-93084efb2777"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.Trash-0': 0,\n",
              " '.file-revisions-by-id': 1,\n",
              " '.shortcut-targets-by-id': 2,\n",
              " 'MyDrive': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Creation**"
      ],
      "metadata": {
        "id": "jB-Ec7ka8ffB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense"
      ],
      "metadata": {
        "id": "bAkRPtYJ8ZHb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Model\n",
        "model=Sequential()"
      ],
      "metadata": {
        "id": "QRzae4kb8ZR8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Layers\n",
        "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3)))"
      ],
      "metadata": {
        "id": "lAu_PH_q81JB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2)))"
      ],
      "metadata": {
        "id": "wG3sWdrW81Tz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Flatten())"
      ],
      "metadata": {
        "id": "YcLBmqyy81eW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Dense Layers\n",
        "model.add(Dense(300,activation='relu'))\n",
        "model.add(Dense(150,activation='relu'))\n",
        "model.add(Dense(4,activation='softmax'))"
      ],
      "metadata": {
        "id": "QfnpanZ79BaJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xgYQcB4G9Bjj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Model Generator\n",
        "model.fit_generator(x_train,steps_per_epoch=len(x_train),epochs=10,validation_data=x_test,validation_steps=len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAzOspgo9Bsy",
        "outputId": "813d13a9-6cd2-4214-d0ac-4cf7e08a8662"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 136s 39s/step - loss: 0.4020 - accuracy: 0.7481 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 18s 6s/step - loss: 1.4864e-10 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 23s 9s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 18s 6s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 19s 6s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 18s 6s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 18s 6s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 18s 7s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 18s 7s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 23s 8s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9950d4310>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the Model**"
      ],
      "metadata": {
        "id": "hnIoCK3y_TIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('asl_model_84_54.h5')"
      ],
      "metadata": {
        "id": "DmiP-ewQ_Qrx"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}